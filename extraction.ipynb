{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4263d88",
   "metadata": {},
   "source": [
    "# Phase 1\n",
    "## Dataset\n",
    "AG News\n",
    "## Metrics: \n",
    "1. Acc\n",
    "2. Brier Score\n",
    "3. Expected Calibration Error (ECE)\n",
    "4. Expected Reducble Uncertatinty (ERU)\n",
    "5. Predictive Discrepecy (OOD AUROC) - P Dist\n",
    "6. Predictive Dispersion (P-Disp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eadb19bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (3.24.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (2.4.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (3.0.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (4.67.3)\n",
      "Requirement already satisfied: xxhash in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: typer>=0.24.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.24.1)\n",
      "Requirement already satisfied: click>=8.2.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (14.3.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.1.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from scikit-learn) (2.4.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from scikit-learn) (1.17.1)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: transformers in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (5.2.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from transformers) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from transformers) (2026.2.19)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.24.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: typer>=0.24.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from typer-slim->transformers) (0.24.1)\n",
      "Requirement already satisfied: click>=8.2.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from typer>=0.24.0->typer-slim->transformers) (14.3.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n",
      "Requirement already satisfied: torch in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (2.10.0)\n",
      "Requirement already satisfied: filelock in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch) (3.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: torchmetrics in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (1.8.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torchmetrics) (2.4.2)\n",
      "Requirement already satisfied: packaging>17.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torchmetrics) (26.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torchmetrics) (2.10.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torchmetrics) (0.15.3)\n",
      "Requirement already satisfied: typing_extensions in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (3.24.3)\n",
      "Requirement already satisfied: setuptools in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
      "Requirement already satisfied: matplotlib in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib) (12.1.1)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from seaborn) (2.4.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from seaborn) (3.0.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from seaborn) (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.1.1)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kakuryu/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install torchmetrics\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0702a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f15c84",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 1. Environment Setup\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34e897fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Apple MPS (M2 accelerated)\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "def setup_environment():\n",
    "    set_seed(42)\n",
    "    os.makedirs(\"data/\", exist_ok=True)\n",
    "    os.makedirs(\"reports/phase1/\", exist_ok=True)\n",
    "    print(\"Environment setup complete. Seed=42.\")\n",
    "\n",
    "# MPS detection + speed tweaks\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✅ Using Apple MPS (M2 accelerated)\")\n",
    "    torch.set_float32_matmul_precision('high')   # critical for speed on MPS\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️  Running on CPU – will be slow\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996374e4",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 2. Data Pipeline\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6b68cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    def tokenize_func(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    ag_news = load_dataset(\"ag_news\", cache_dir=\"data/\")\n",
    "    train_val_split = ag_news[\"train\"].train_test_split(test_size=6000, seed=42)\n",
    "    train_data = train_val_split[\"train\"]\n",
    "    val_data = train_val_split[\"test\"]\n",
    "    clean_test = ag_news[\"test\"]\n",
    "\n",
    "    def make_noisy_test(clean_test, noise_rate=0.2, seed=42):\n",
    "        rng = random.Random(seed)\n",
    "        n = len(clean_test)\n",
    "        k = int(n * noise_rate)\n",
    "        flip_index = rng.sample(range(n), k)\n",
    "\n",
    "        def inject_noise(example, idx):\n",
    "            is_flipped = idx in flip_index\n",
    "            if is_flipped:\n",
    "                original_label = example[\"label\"]\n",
    "                possible_flips = [l for l in range(4) if l != original_label]\n",
    "                example[\"label\"] = random.choice(possible_flips)\n",
    "            example[\"is_flipped\"] = is_flipped\n",
    "            return example\n",
    "        return clean_test.map(inject_noise, with_indices=True)\n",
    "\n",
    "    noisy_test = make_noisy_test(clean_test, noise_rate=0.2, seed=42)\n",
    "\n",
    "    imdb = load_dataset(\"imdb\", split=\"test\", cache_dir=\"data/\")\n",
    "    ood_test = imdb.select(range(3000))\n",
    "    ood_test = ood_test.map(lambda x: {\"label\": 0})\n",
    "\n",
    "    # Tokenize (cached)\n",
    "    encoded_train = train_data.map(tokenize_func, batched=True).with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    encoded_val = val_data.map(tokenize_func, batched=True).with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    encoded_clean_test = clean_test.map(tokenize_func, batched=True).with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    encoded_noisy_test = noisy_test.map(tokenize_func, batched=True).with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\", \"is_flipped\"])\n",
    "    encoded_ood_test = ood_test.map(tokenize_func, batched=True).with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    return encoded_train, encoded_val, encoded_clean_test, encoded_noisy_test, encoded_ood_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cba2fc",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 3. Model & Loss\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d3b84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidentialHead(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        raw = self.linear(features)\n",
    "        evidence = F.softplus(raw)\n",
    "        alpha = evidence + 1.0\n",
    "        return evidence, alpha\n",
    "\n",
    "class BertWithEvidentialHead(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.bert.eval()\n",
    "        self.head = EvidentialHead(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        self.bert.eval()\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        cls = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.head(cls)\n",
    "\n",
    "def get_expected_probs(alpha):\n",
    "    S = alpha.sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "    return alpha / S\n",
    "\n",
    "def brier_score_loss(alpha, target, num_classes=4):\n",
    "    p_hat = get_expected_probs(alpha)\n",
    "    one_hot = torch.zeros(target.size(0), num_classes, device=target.device)\n",
    "    one_hot.scatter_(1, target.unsqueeze(1), 1)\n",
    "    return torch.mean(torch.sum((p_hat - one_hot) ** 2, dim=1))\n",
    "\n",
    "def quick_gradient_check(model, num_classes=4):\n",
    "    model.train()\n",
    "    input_ids = torch.randint(0, 30522, (2, 8), device=device)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    target = torch.randint(0, num_classes, (2,), device=device)\n",
    "    _, alpha = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    loss = brier_score_loss(alpha, target, num_classes=num_classes)\n",
    "    assert torch.isfinite(loss).all()\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    assert all(p.grad is not None for p in model.head.parameters())\n",
    "    print(\"Gradient check passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f449510",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 4. Metrics Implementation\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4deb8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_accuracy(alpha, target):\n",
    "    p_mean = get_expected_probs(alpha)\n",
    "    preds = torch.argmax(p_mean, dim=1)\n",
    "    return (preds == target).float().mean()\n",
    "\n",
    "def _ece_from_probs(probs, target, n_bins=15):\n",
    "    conf, preds = probs.max(dim=1)\n",
    "    acc = preds.eq(target).float()\n",
    "    bins = torch.linspace(0, 1, n_bins + 1, device=probs.device)\n",
    "    ece = torch.zeros(1, device=probs.device)\n",
    "    for i in range(n_bins):\n",
    "        mask = (conf >= bins[i]) & (conf <= bins[i + 1]) if i == n_bins - 1 else (conf >= bins[i]) & (conf < bins[i + 1])\n",
    "        if mask.any():\n",
    "            ece += mask.float().mean() * (conf[mask].mean() - acc[mask].mean()).abs()\n",
    "    return ece.squeeze(0)\n",
    "\n",
    "def eru_mutual_information(alpha, eps=1e-12):\n",
    "    S = alpha.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "    p_mean = alpha / S\n",
    "    predictive_entropy = -torch.sum(p_mean * torch.log(p_mean.clamp_min(eps)), dim=1)\n",
    "    expected_entropy = -torch.sum((alpha / S) * (torch.digamma(alpha + 1) - torch.digamma(S + 1)), dim=1)\n",
    "    return predictive_entropy - expected_entropy\n",
    "\n",
    "def _kl_p_mean_expected(p_mean, expected_p, eps=1e-12):\n",
    "    if expected_p.dim() == 1:\n",
    "        expected_p = expected_p.unsqueeze(0)\n",
    "    expected_p = expected_p / expected_p.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "    return torch.sum(p_mean.clamp_min(eps) * (torch.log(p_mean.clamp_min(eps)) - torch.log(expected_p.clamp_min(eps))), dim=1)\n",
    "\n",
    "def p_disc_auroc(alpha_clean, alpha_ood):\n",
    "    try:\n",
    "        from torchmetrics.classification import BinaryAUROC\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install torchmetrics for AUROC calculation\")\n",
    "        \n",
    "    p_mean_clean = get_expected_probs(alpha_clean)\n",
    "    p_mean_ood = get_expected_probs(alpha_ood)\n",
    "    expected_p = p_mean_clean.mean(dim=0) # Aggregate expected probability\n",
    "    \n",
    "    scores_clean = _kl_p_mean_expected(p_mean_clean, expected_p)\n",
    "    scores_ood = _kl_p_mean_expected(p_mean_ood, expected_p)\n",
    "    \n",
    "    scores = torch.cat([scores_clean, scores_ood], dim=0)\n",
    "    labels = torch.cat([torch.zeros_like(scores_clean, dtype=torch.long), torch.ones_like(scores_ood, dtype=torch.long)], dim=0)\n",
    "    \n",
    "    metric = BinaryAUROC().to(scores.device)\n",
    "    return metric(scores, labels)\n",
    "\n",
    "def p_disp_cohens_d(alpha_clean, alpha_noisy, eps=1e-12):\n",
    "    p_mean_clean = get_expected_probs(alpha_clean)\n",
    "    p_mean_noisy = get_expected_probs(alpha_noisy)\n",
    "    \n",
    "    ent_clean = -torch.sum(p_mean_clean * torch.log(p_mean_clean.clamp_min(eps)), dim=1)\n",
    "    ent_noisy = -torch.sum(p_mean_noisy * torch.log(p_mean_noisy.clamp_min(eps)), dim=1)\n",
    "    \n",
    "    v1, v2 = ent_clean.var(unbiased=False), ent_noisy.var(unbiased=False)\n",
    "    n1, n2 = ent_clean.numel(), ent_noisy.numel()\n",
    "    pooled_std = torch.sqrt(((n1 * v1) + (n2 * v2)) / (n1 + n2) + eps)\n",
    "    return (ent_noisy.mean() - ent_clean.mean()) / pooled_std # Noisy should have higher entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42a0e7",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 5. Training Loop & Evaluation\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b6651249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tensors(batch):\n",
    "#     return batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
    "\n",
    "# def get_all_alphas_and_targets(model, dataloader):\n",
    "#     model.eval()\n",
    "#     all_alphas, all_targets = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             input_ids, attn_mask, labels = extract_tensors(batch)\n",
    "#             _, alpha = model(input_ids=input_ids, attention_mask=attn_mask)\n",
    "#             all_alphas.append(alpha.cpu())   # move to CPU early to free MPS memory\n",
    "#             all_targets.append(labels.cpu())\n",
    "#     return torch.cat(all_alphas).to(device), torch.cat(all_targets).to(device)\n",
    "\n",
    "# def train_and_evaluate():\n",
    "#     setup_environment()                     # ← moved to top\n",
    "#     train_data, val_data, clean_test, noisy_test, ood_test = prepare_datasets()\n",
    "    \n",
    "#     # Optimized DataLoaders for MPS\n",
    "#     train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "#     val_loader   = DataLoader(val_data,   batch_size=256, shuffle=False, num_workers=0, persistent_workers=False)\n",
    "\n",
    "#     model = BertWithEvidentialHead(num_classes=4).to(device)   # ← ONLY ONCE\n",
    "#     quick_gradient_check(model)\n",
    "    \n",
    "#     optimizer = optim.AdamW(model.head.parameters(), lr=5e-4)\n",
    "#     epochs = 5\n",
    "\n",
    "#     print(\"\\n--- Starting Training ---\")\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         start_time = time.time()\n",
    "#         model.train()\n",
    "#         total_loss, total_count = 0.0, 0\n",
    "        \n",
    "#         for batch in train_loader:\n",
    "#             input_ids, attn_mask, labels = extract_tensors(batch)\n",
    "#             _, alpha = model(input_ids=input_ids, attention_mask=attn_mask)\n",
    "            \n",
    "#             loss = brier_score_loss(alpha, labels, num_classes=4)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_loss += loss.item() * labels.size(0)\n",
    "#             total_count += labels.size(0)\n",
    "        \n",
    "#         # LIGHT validation (only accuracy, fast)\n",
    "#         with torch.no_grad():\n",
    "#             val_alphas, val_targets = get_all_alphas_and_targets(model, val_loader)\n",
    "#             val_acc = base_accuracy(val_alphas, val_targets)\n",
    "        \n",
    "#         epoch_time = time.time() - start_time\n",
    "#         print(f\"Epoch {epoch}/{epochs} | Train Loss: {total_loss/total_count:.4f} | Val Acc: {val_acc:.4f} | AvgTime per Epoch: {epoch_time:.1f}s\")\n",
    "\n",
    "#     print(\"\\n--- Running Full Metric Suite (once) ---\")\n",
    "#     # Final heavy metrics (larger batch for speed)\n",
    "#     test_loader = lambda ds: DataLoader(ds, batch_size=256, shuffle=False, num_workers=0)\n",
    "#     alpha_clean, targets_clean = get_all_alphas_and_targets(model, test_loader(clean_test))\n",
    "#     alpha_noisy, _ = get_all_alphas_and_targets(model, test_loader(noisy_test))\n",
    "#     alpha_ood, _   = get_all_alphas_and_targets(model, test_loader(ood_test))\n",
    "\n",
    "#     # (rest of your metric prints unchanged)\n",
    "#     clean_acc = base_accuracy(alpha_clean, targets_clean)\n",
    "#     clean_brier = brier_score_loss(alpha_clean, targets_clean, num_classes=4)\n",
    "#     clean_ece = _ece_from_probs(get_expected_probs(alpha_clean), targets_clean, n_bins=15)\n",
    "#     clean_eru = eru_mutual_information(alpha_clean).mean()\n",
    "#     auroc = p_disc_auroc(alpha_clean, alpha_ood)\n",
    "#     cohens_d = p_disp_cohens_d(alpha_clean, alpha_noisy)\n",
    "\n",
    "#     print(f\"Clean Accuracy:  {clean_acc:.4f} (Target > 0.90)\")\n",
    "#     print(f\"Brier Score:     {clean_brier:.4f}\")\n",
    "#     print(f\"ECE:             {clean_ece:.4f} (Target < 0.05)\")\n",
    "#     print(f\"ERU (Mean):      {clean_eru:.4f}\")\n",
    "#     print(f\"P-Disc (AUROC):  {auroc:.4f} (Target > 0.85)\")\n",
    "#     print(f\"P-Disp (Cohen's d): {cohens_d:.4f} (Target > 1.3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "efdec04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(batch):\n",
    "    return batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
    "\n",
    "def get_all_alphas_and_targets(model, dataloader):\n",
    "    model.eval()\n",
    "    all_alphas, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attn_mask, labels = extract_tensors(batch)\n",
    "            _, alpha = model(input_ids=input_ids, attention_mask=attn_mask)\n",
    "            all_alphas.append(alpha.cpu())   # move to CPU early to free MPS memory\n",
    "            all_targets.append(labels.cpu())\n",
    "    return torch.cat(all_alphas).to(device), torch.cat(all_targets).to(device)\n",
    "\n",
    "def train_and_evaluate():\n",
    "    setup_environment()\n",
    "    train_data, val_data, clean_test, noisy_test, ood_test = prepare_datasets()\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_data,   batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = BertWithEvidentialHead(num_classes=4).to(device)\n",
    "    quick_gradient_check(model)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.head.parameters(), lr=5e-4)\n",
    "    epochs = 5\n",
    "\n",
    "    # === COLLECT HISTORY FOR PLOTS ===\n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    epoch_times = []\n",
    "\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_loss, total_count = 0.0, 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids, attn_mask, labels = extract_tensors(batch)\n",
    "            _, alpha = model(input_ids=input_ids, attention_mask=attn_mask)\n",
    "            \n",
    "            loss = brier_score_loss(alpha, labels, num_classes=4)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_count += labels.size(0)\n",
    "        \n",
    "        # Light validation\n",
    "        with torch.no_grad():\n",
    "            val_alphas, val_targets = get_all_alphas_and_targets(model, val_loader)\n",
    "            val_acc = base_accuracy(val_alphas, val_targets).item()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        train_losses.append(total_loss / total_count)\n",
    "        val_accs.append(val_acc)\n",
    "        epoch_times.append(epoch_time)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} | Loss: {train_losses[-1]:.4f} | Val Acc: {val_acc:.4f} | Time: {epoch_time:.1f}s\")\n",
    "\n",
    "    print(\"\\n--- Running Full Metric Suite ---\")\n",
    "    test_loader = lambda ds: DataLoader(ds, batch_size=256, shuffle=False, num_workers=0)\n",
    "    alpha_clean, targets_clean = get_all_alphas_and_targets(model, test_loader(clean_test))\n",
    "    alpha_noisy, _ = get_all_alphas_and_targets(model, test_loader(noisy_test))\n",
    "    alpha_ood, _   = get_all_alphas_and_targets(model, test_loader(ood_test))\n",
    "\n",
    "    # Core metrics\n",
    "    clean_acc = base_accuracy(alpha_clean, targets_clean).item()\n",
    "    clean_brier = brier_score_loss(alpha_clean, targets_clean, num_classes=4).item()\n",
    "    clean_ece = _ece_from_probs(get_expected_probs(alpha_clean), targets_clean).item()\n",
    "    clean_eru = eru_mutual_information(alpha_clean).mean().item()\n",
    "    auroc = p_disc_auroc(alpha_clean, alpha_ood).item()\n",
    "    cohens_d = p_disp_cohens_d(alpha_clean, alpha_noisy).item()\n",
    "\n",
    "    print(f\"Clean Accuracy:  {clean_acc:.4f}\")\n",
    "    print(f\"Brier Score:     {clean_brier:.4f}\")\n",
    "    print(f\"ECE:             {clean_ece:.4f}\")\n",
    "    print(f\"ERU:             {clean_eru:.4f}\")\n",
    "    print(f\"P-Disc (AUROC):  {auroc:.4f}\")\n",
    "    print(f\"P-Disp (d):      {cohens_d:.4f}\")\n",
    "\n",
    "    # === VISUALIZE ===\n",
    "    metrics_dict = {\n",
    "        \"Accuracy\": clean_acc,\n",
    "        \"Brier\": clean_brier,\n",
    "        \"ECE\": clean_ece,\n",
    "        \"ERU\": clean_eru,\n",
    "        \"P-Disc\": auroc,\n",
    "        \"P-Disp\": cohens_d\n",
    "    }\n",
    "    save_phase1_visualizations(train_losses, val_accs, epoch_times,\n",
    "                               alpha_clean, targets_clean,\n",
    "                               alpha_noisy, alpha_ood,\n",
    "                               metrics_dict)\n",
    "    \n",
    "    print(f\"\\n✅ All plots saved to reports/phase1/\")\n",
    "\n",
    "def save_phase1_visualizations(train_losses, val_accs, epoch_times,\n",
    "                               alpha_clean, targets_clean,\n",
    "                               alpha_noisy, alpha_ood,\n",
    "                               metrics_dict, save_dir=\"reports/phase1\"):\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # 1. Training curves\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    axs[0].plot(epochs, train_losses, 'b-o', linewidth=2)\n",
    "    axs[0].set_title(\"Train Loss (Brier)\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    \n",
    "    axs[1].plot(epochs, val_accs, 'g-o', linewidth=2)\n",
    "    axs[1].set_title(\"Validation Accuracy\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    \n",
    "    axs[2].bar(epochs, epoch_times, color='orange')\n",
    "    axs[2].set_title(\"Time per Epoch (seconds)\")\n",
    "    axs[2].set_xlabel(\"Epoch\")\n",
    "    axs[2].set_ylabel(\"Seconds\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/training_curves.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Reliability diagram (ECE)\n",
    "    p_mean = get_expected_probs(alpha_clean)\n",
    "    conf, preds = p_mean.max(dim=1)\n",
    "    acc = preds.eq(targets_clean).float()\n",
    "    bins = torch.linspace(0, 1, 16, device=conf.device)\n",
    "    bin_acc, bin_conf, bin_count = [], [], []\n",
    "    for i in range(15):\n",
    "        mask = (conf >= bins[i]) & (conf < bins[i+1])\n",
    "        if mask.any():\n",
    "            bin_conf.append(conf[mask].mean().item())\n",
    "            bin_acc.append(acc[mask].mean().item())\n",
    "            bin_count.append(mask.sum().item())\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    plt.scatter(bin_conf, bin_acc, s=80, c=bin_count, cmap='Blues', edgecolors='black')\n",
    "    plt.colorbar(label='Samples per bin')\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"Reliability Diagram (ECE = {metrics_dict['ECE']:.4f})\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_dir}/reliability_diagram.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Entropy distribution (P-Disp)\n",
    "    ent_clean = -torch.sum(get_expected_probs(alpha_clean) * \n",
    "                          torch.log(get_expected_probs(alpha_clean).clamp_min(1e-12)), dim=1).cpu().numpy()\n",
    "    ent_noisy = -torch.sum(get_expected_probs(alpha_noisy) * \n",
    "                          torch.log(get_expected_probs(alpha_noisy).clamp_min(1e-12)), dim=1).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=[ent_clean, ent_noisy], notch=True)\n",
    "    plt.xticks([0, 1], ['Clean Test', '20% Noisy'])\n",
    "    plt.ylabel(\"Predictive Entropy\")\n",
    "    plt.title(f\"P-Disp: Cohen's d = {metrics_dict['P-Disp']:.3f}\")\n",
    "    plt.savefig(f\"{save_dir}/entropy_distribution.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 4. KL discrepancy histogram (P-Disc)\n",
    "    p_mean_clean = get_expected_probs(alpha_clean)\n",
    "    p_mean_ood = get_expected_probs(alpha_ood)\n",
    "    expected_p = p_mean_clean.mean(dim=0)\n",
    "    kl_clean = _kl_p_mean_expected(p_mean_clean, expected_p).cpu().numpy()\n",
    "    kl_ood = _kl_p_mean_expected(p_mean_ood, expected_p).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.histplot(kl_clean, kde=True, color='blue', label='Clean (ID)', alpha=0.7, bins=50)\n",
    "    sns.histplot(kl_ood, kde=True, color='red', label='OOD (IMDB)', alpha=0.7, bins=50)\n",
    "    plt.axvline(kl_clean.mean(), color='blue', linestyle='--')\n",
    "    plt.axvline(kl_ood.mean(), color='red', linestyle='--')\n",
    "    plt.xlabel(\"KL(p_mean || E[p])\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"P-Disc: AUROC = {metrics_dict['P-Disc']:.4f}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_dir}/kl_discrepancy.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 5. Metrics summary bar chart\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': list(metrics_dict.keys()),\n",
    "        'Value': list(metrics_dict.values())\n",
    "    })\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['green' if v > 0.9 else 'red' if k in ['P-Disc','P-Disp'] else 'orange' \n",
    "              for k, v in metrics_dict.items()]\n",
    "    bars = plt.bar(df['Metric'], df['Value'], color=colors)\n",
    "    plt.axhline(0.90, color='gray', linestyle='--', label='Target threshold (0.90)')\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Phase 1 Final Metrics\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    for bar, val in zip(bars, df['Value']):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{val:.3f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/metrics_summary.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Bonus: one-page markdown report\n",
    "    with open(f\"{save_dir}/phase1_report.md\", \"w\") as f:\n",
    "        f.write(\"# Phase 1 – UQ Sanity Check Report\\n\\n\")\n",
    "        f.write(f\"**Dataset**: AG News (clean) + IMDB (OOD)\\n\")\n",
    "        f.write(f\"**Model**: Frozen bert-base-uncased + Evidential Brier head\\n\\n\")\n",
    "        f.write(\"## Final Metrics\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Visuals\\n\")\n",
    "        f.write(\"![Training Curves](training_curves.png)\\n\")\n",
    "        f.write(\"![Reliability](reliability_diagram.png)\\n\")\n",
    "        f.write(\"![Entropy](entropy_distribution.png)\\n\")\n",
    "        f.write(\"![KL](kl_discrepancy.png)\\n\")\n",
    "        f.write(\"![Metrics](metrics_summary.png)\\n\")\n",
    "        f.write(\"\\n**All asserts passed** ✅\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9af1d3",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 5. Run Code\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "124241d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete. Seed=42.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 2048.27it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed.\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/5 | Loss: 0.3744 | Val Acc: 0.8602 | Time: 2162.2s\n",
      "Epoch 2/5 | Loss: 0.2496 | Val Acc: 0.8723 | Time: 2218.4s\n",
      "Epoch 3/5 | Loss: 0.2212 | Val Acc: 0.8785 | Time: 2010.1s\n",
      "Epoch 4/5 | Loss: 0.2064 | Val Acc: 0.8812 | Time: 1899.0s\n",
      "Epoch 5/5 | Loss: 0.1970 | Val Acc: 0.8817 | Time: 1961.1s\n",
      "\n",
      "--- Running Full Metric Suite ---\n",
      "Clean Accuracy:  0.8859\n",
      "Brier Score:     0.2001\n",
      "ECE:             0.1322\n",
      "ERU:             0.0783\n",
      "P-Disc (AUROC):  0.0792\n",
      "P-Disp (d):      0.0000\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`Import tabulate` failed.  Use pip or conda to install the tabulate package.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages/pandas/compat/_optional.py:158\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, min_version, errors)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/importlib/__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tabulate'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# === VISUALIZE ===\u001b[39;00m\n\u001b[32m     85\u001b[39m metrics_dict = {\n\u001b[32m     86\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m: clean_acc,\n\u001b[32m     87\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBrier\u001b[39m\u001b[33m\"\u001b[39m: clean_brier,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mP-Disp\u001b[39m\u001b[33m\"\u001b[39m: cohens_d\n\u001b[32m     92\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43msave_phase1_visualizations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_accs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m                           \u001b[49m\u001b[43malpha_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_clean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m                           \u001b[49m\u001b[43malpha_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_ood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mmetrics_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ All plots saved to reports/phase1/\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 213\u001b[39m, in \u001b[36msave_phase1_visualizations\u001b[39m\u001b[34m(train_losses, val_accs, epoch_times, alpha_clean, targets_clean, alpha_noisy, alpha_ood, metrics_dict, save_dir)\u001b[39m\n\u001b[32m    211\u001b[39m f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m**Model**: Frozen bert-base-uncased + Evidential Brier head\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m f.write(\u001b[33m\"\u001b[39m\u001b[33m## Final Metrics\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m f.write(\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    214\u001b[39m f.write(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m## Visuals\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m f.write(\u001b[33m\"\u001b[39m\u001b[33m![Training Curves](training_curves.png)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages/pandas/core/frame.py:2983\u001b[39m, in \u001b[36mDataFrame.to_markdown\u001b[39m\u001b[34m(self, buf, mode, index, storage_options, **kwargs)\u001b[39m\n\u001b[32m   2981\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mtablefmt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2982\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mshowindex\u001b[39m\u001b[33m\"\u001b[39m, index)\n\u001b[32m-> \u001b[39m\u001b[32m2983\u001b[39m tabulate = \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabulate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2984\u001b[39m result = tabulate.tabulate(\u001b[38;5;28mself\u001b[39m, **kwargs)\n\u001b[32m   2985\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/Epistemic Ranker/.venv/lib/python3.13/site-packages/pandas/compat/_optional.py:161\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, min_version, errors)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: `Import tabulate` failed.  Use pip or conda to install the tabulate package."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42e2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
